<?xml version="1.0" encoding="UTF-8"?>
<zabbix_export>
    <version>4.4</version>
    <date>2022-08-12T07:01:55Z</date>
    <groups>
        <group>
            <name>Templates</name>
        </group>
    </groups>
    <templates>
        <template>
            <template>Template Ceph</template>
            <name>Template Ceph</name>
            <groups>
                <group>
                    <name>Templates</name>
                </group>
            </groups>
            <applications>
                <application>
                    <name>Ceph Cluster</name>
                </application>
                <application>
                    <name>Ceph pg</name>
                </application>
                <application>
                    <name>Ceph Space</name>
                </application>
            </applications>
            <items>
                <item>
                    <name>Ceph PG activating</name>
                    <type>TRAP</type>
                    <key>ceph.activating</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG active</name>
                    <type>TRAP</type>
                    <key>ceph.active</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG backfilling</name>
                    <type>TRAP</type>
                    <key>ceph.backfilling</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG backfill_toofull</name>
                    <type>TRAP</type>
                    <key>ceph.backfill_toofull</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG backfill_unfound</name>
                    <type>TRAP</type>
                    <key>ceph.backfill_unfound</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG wait-backfill</name>
                    <type>TRAP</type>
                    <key>ceph.backfill_wait</key>
                    <delay>0</delay>
                    <description>The placement group is waiting in line to start backfill.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG clean</name>
                    <type>TRAP</type>
                    <key>ceph.clean</key>
                    <delay>0</delay>
                    <description>Ceph replicated all objects in the placement group the correct number of times.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG creating</name>
                    <type>TRAP</type>
                    <key>ceph.creating</key>
                    <delay>0</delay>
                    <description>Ceph is still creating the placement group.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG deep</name>
                    <type>TRAP</type>
                    <key>ceph.deep</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG degraded</name>
                    <type>TRAP</type>
                    <key>ceph.degraded</key>
                    <delay>0</delay>
                    <description>Ceph has not replicated some objects in the placement group the correct number of times yet.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                    <triggers>
                        <trigger>
                            <expression>{last(0)}&gt;0</expression>
                            <name>Ceph cluster has degraded PGs</name>
                            <priority>WARNING</priority>
                            <description>Ceph has not replicated some objects in the placement group the correct number of times yet.</description>
                        </trigger>
                    </triggers>
                </item>
                <item>
                    <name>Ceph PG degraded %</name>
                    <type>CALCULATED</type>
                    <key>ceph.degraded_percent</key>
                    <delay>30</delay>
                    <value_type>FLOAT</value_type>
                    <units>%</units>
                    <params>100*last(&quot;ceph.degraded&quot;)/last(&quot;ceph.pgtotal&quot;)</params>
                    <description>Ceph has not replicated some objects in the placement group the correct number of times yet.</description>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG down</name>
                    <type>TRAP</type>
                    <key>ceph.down</key>
                    <delay>0</delay>
                    <description>A replica with necessary data is down, so the placement group is offline.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                    <triggers>
                        <trigger>
                            <expression>{last(0)}&gt;0</expression>
                            <name>Ceph cluster has down PGs</name>
                            <priority>AVERAGE</priority>
                            <description>At least a replica with necessary data is down, so the placement group is offline.</description>
                        </trigger>
                    </triggers>
                </item>
                <item>
                    <name>Ceph PG forced_backfill</name>
                    <type>TRAP</type>
                    <key>ceph.forced_backfill</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG forced_recovery</name>
                    <type>TRAP</type>
                    <key>ceph.forced_recovery</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Full Ratio Space</name>
                    <type>CALCULATED</type>
                    <key>ceph.fullratio</key>
                    <delay>5m</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <params>last(&quot;ceph.spacetotal&quot;)*{$FULLRATIO}/100</params>
                    <description>http://docs.ceph.com/docs/master/rados/troubleshooting/troubleshooting-osd/&#13;
NO FREE DRIVE SPACE&#13;
&#13;
Ceph prevents you from writing to a full OSD so that you don’t lose data. In an operational cluster, you should receive a warning when your cluster is getting near its full ratio. The mon osd full ratio defaults to 0.95, or 95% of capacity before it stops clients from writing data. The mon osd nearfull ratio defaults to 0.85, or 85% of capacity when it generates a health warning.&#13;
&#13;
Full cluster issues usually arise when testing how Ceph handles an OSD failure on a small cluster. When one node has a high percentage of the cluster’s data, the cluster can easily eclipse its nearfull and full ratio immediately. If you are testing how Ceph reacts to OSD failures on a small cluster, you should leave ample free disk space and consider temporarily lowering the mon osd full ratio and mon osd nearfull ratio.</description>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                    <triggers>
                        <trigger>
                            <expression>{timeleft(86400,,0)}&lt;1d</expression>
                            <name>Ceph Cluster Used Space will reach FullRatio Mark less than in a day</name>
                            <priority>HIGH</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach FullRatio Mark less than in an hour</name>
                                    <expression>{Template Ceph:ceph.fullratio.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(3600,,0)}&lt;1h</expression>
                            <name>Ceph Cluster Used Space will reach FullRatio Mark less than in an hour</name>
                            <priority>DISASTER</priority>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(604800,,0)}&lt;1w</expression>
                            <name>Ceph Cluster Used Space will reach FullRatio Mark less than in a week</name>
                            <priority>WARNING</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach FullRatio Mark less than in a day</name>
                                    <expression>{Template Ceph:ceph.fullratio.timeleft(86400,,0)}&lt;1d</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach FullRatio Mark less than in an hour</name>
                                    <expression>{Template Ceph:ceph.fullratio.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                    </triggers>
                </item>
                <item>
                    <name>Ceph Full Ratio %</name>
                    <type>CALCULATED</type>
                    <key>ceph.fullratioprcnt</key>
                    <delay>5m</delay>
                    <value_type>FLOAT</value_type>
                    <units>%</units>
                    <params>100*last(&quot;ceph.fullratio&quot;)/last(&quot;ceph.spacetotal&quot;)</params>
                    <description>http://docs.ceph.com/docs/master/rados/troubleshooting/troubleshooting-osd/&#13;
NO FREE DRIVE SPACE&#13;
&#13;
Ceph prevents you from writing to a full OSD so that you don’t lose data. In an operational cluster, you should receive a warning when your cluster is getting near its full ratio. The mon osd full ratio defaults to 0.95, or 95% of capacity before it stops clients from writing data. The mon osd nearfull ratio defaults to 0.85, or 85% of capacity when it generates a health warning.&#13;
&#13;
Full cluster issues usually arise when testing how Ceph handles an OSD failure on a small cluster. When one node has a high percentage of the cluster’s data, the cluster can easily eclipse its nearfull and full ratio immediately. If you are testing how Ceph reacts to OSD failures on a small cluster, you should leave ample free disk space and consider temporarily lowering the mon osd full ratio and mon osd nearfull ratio.</description>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Cluster health</name>
                    <key>ceph.health[{$CLUSTER_NAME},health,{HOST.HOST}]</key>
                    <delay>30s</delay>
                    <description>HEALTH_OK: 1&#13;
HEALTH_WARN: 2&#13;
HEALTH_ERR: 0</description>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                    <valuemap>
                        <name>Ceph Cluster Status</name>
                    </valuemap>
                </item>
                <item>
                    <name>Ceph PG incomplete</name>
                    <type>TRAP</type>
                    <key>ceph.incomplete</key>
                    <delay>0</delay>
                    <description>Ceph detects that a placement group is missing a necessary period of history from its log. If you see this state, report a bug, and try to start any failed OSDs that may contain the needed information.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG inconsistent</name>
                    <type>TRAP</type>
                    <key>ceph.inconsistent</key>
                    <delay>0</delay>
                    <description>Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica after recovery finished, etc.).</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph MON Count</name>
                    <type>TRAP</type>
                    <key>ceph.moncount</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph NearFull Ratio Space</name>
                    <type>CALCULATED</type>
                    <key>ceph.nearfullratio</key>
                    <delay>5m</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <params>(last(&quot;ceph.spacetotal&quot;)-last(&quot;ceph.spacetotal&quot;)/last(&quot;ceph.osdcount&quot;))*{$FULLRATIO}/100</params>
                    <description>http://docs.ceph.com/docs/master/rados/troubleshooting/troubleshooting-osd/&#13;
NO FREE DRIVE SPACE&#13;
&#13;
Ceph prevents you from writing to a full OSD so that you don’t lose data. In an operational cluster, you should receive a warning when your cluster is getting near its full ratio. The mon osd full ratio defaults to 0.95, or 95% of capacity before it stops clients from writing data. The mon osd nearfull ratio defaults to 0.85, or 85% of capacity when it generates a health warning.&#13;
&#13;
Full cluster issues usually arise when testing how Ceph handles an OSD failure on a small cluster. When one node has a high percentage of the cluster’s data, the cluster can easily eclipse its nearfull and full ratio immediately. If you are testing how Ceph reacts to OSD failures on a small cluster, you should leave ample free disk space and consider temporarily lowering the mon osd full ratio and mon osd nearfull ratio.</description>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                    <triggers>
                        <trigger>
                            <expression>{timeleft(86400,,0)}&lt;1d</expression>
                            <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a day</name>
                            <priority>HIGH</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in an hour</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(2592000,,0)}&lt;30d</expression>
                            <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a month</name>
                            <priority>INFO</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a day</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(86400,,0)}&lt;1d</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in an hour</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a week</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(604800,,0)}&lt;1w</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(3600,,0)}&lt;1h</expression>
                            <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in an hour</name>
                            <priority>DISASTER</priority>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(604800,,0)}&lt;1w</expression>
                            <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a week</name>
                            <priority>WARNING</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in a day</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(86400,,0)}&lt;1d</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach NearFullRatio Mark less than in an hour</name>
                                    <expression>{Template Ceph:ceph.nearfullratio.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                    </triggers>
                </item>
                <item>
                    <name>Ceph NearFull Ratio %</name>
                    <type>CALCULATED</type>
                    <key>ceph.nearfullratioprcnt</key>
                    <delay>5m</delay>
                    <value_type>FLOAT</value_type>
                    <units>%</units>
                    <params>100*last(&quot;ceph.nearfullratio&quot;)/last(&quot;ceph.spacetotal&quot;)</params>
                    <description>http://docs.ceph.com/docs/master/rados/troubleshooting/troubleshooting-osd/&#13;
NO FREE DRIVE SPACE&#13;
&#13;
Ceph prevents you from writing to a full OSD so that you don’t lose data. In an operational cluster, you should receive a warning when your cluster is getting near its full ratio. The mon osd full ratio defaults to 0.95, or 95% of capacity before it stops clients from writing data. The mon osd nearfull ratio defaults to 0.85, or 85% of capacity when it generates a health warning.&#13;
&#13;
Full cluster issues usually arise when testing how Ceph handles an OSD failure on a small cluster. When one node has a high percentage of the cluster’s data, the cluster can easily eclipse its nearfull and full ratio immediately. If you are testing how Ceph reacts to OSD failures on a small cluster, you should leave ample free disk space and consider temporarily lowering the mon osd full ratio and mon osd nearfull ratio.</description>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph OSD in</name>
                    <type>TRAP</type>
                    <key>ceph.num_in_osds</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph OSD up</name>
                    <type>TRAP</type>
                    <key>ceph.num_up_osds</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Operation Read</name>
                    <type>TRAP</type>
                    <key>ceph.opsread</key>
                    <delay>0</delay>
                    <units>op/s</units>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Operation Write</name>
                    <type>TRAP</type>
                    <key>ceph.opswrite</key>
                    <delay>0</delay>
                    <units>op/s</units>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph OSD Count</name>
                    <type>TRAP</type>
                    <key>ceph.osdcount</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph OSD in %</name>
                    <type>CALCULATED</type>
                    <key>ceph.osd_in</key>
                    <delay>30</delay>
                    <value_type>FLOAT</value_type>
                    <units>%</units>
                    <params>100*last(&quot;ceph.num_in_osds&quot;)/last(&quot;ceph.osdcount&quot;)</params>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph OSD up %</name>
                    <type>CALCULATED</type>
                    <key>ceph.osd_up</key>
                    <delay>30</delay>
                    <value_type>FLOAT</value_type>
                    <units>%</units>
                    <params>100*last(&quot;ceph.num_up_osds&quot;)/last(&quot;ceph.osdcount&quot;)</params>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG peered</name>
                    <type>TRAP</type>
                    <key>ceph.peered</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG peering</name>
                    <type>TRAP</type>
                    <key>ceph.peering</key>
                    <delay>0</delay>
                    <description>The placement group is undergoing the peering process</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG total</name>
                    <type>TRAP</type>
                    <key>ceph.pgtotal</key>
                    <delay>0</delay>
                    <description>Ceph total placement group number.</description>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Read Speed</name>
                    <type>TRAP</type>
                    <key>ceph.rdbps</key>
                    <delay>0</delay>
                    <units>B/s</units>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG recovering</name>
                    <type>TRAP</type>
                    <key>ceph.recovering</key>
                    <delay>0</delay>
                    <description>Ceph is migrating/synchronizing objects and their replicas.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG recovery_toofull</name>
                    <type>TRAP</type>
                    <key>ceph.recovery_toofull</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG recovery_unfound</name>
                    <type>TRAP</type>
                    <key>ceph.recovery_unfound</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG recovery_wait</name>
                    <type>TRAP</type>
                    <key>ceph.recovery_wait</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG remapped</name>
                    <type>TRAP</type>
                    <key>ceph.remapped</key>
                    <delay>0</delay>
                    <description>The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG repair</name>
                    <type>TRAP</type>
                    <key>ceph.repair</key>
                    <delay>0</delay>
                    <description>Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG scrubbing</name>
                    <type>TRAP</type>
                    <key>ceph.scrubbing</key>
                    <delay>0</delay>
                    <description>Ceph is checking the placement group for inconsistencies.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG snaptrim</name>
                    <type>TRAP</type>
                    <key>ceph.snaptrim</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG snaptrim_error</name>
                    <type>TRAP</type>
                    <key>ceph.snaptrim_error</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG snaptrim_wait</name>
                    <type>TRAP</type>
                    <key>ceph.snaptrim_wait</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Space Available</name>
                    <type>TRAP</type>
                    <key>ceph.spaceavail</key>
                    <delay>0</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                    <triggers>
                        <trigger>
                            <expression>{timeleft(86400,,0)}&lt;1h</expression>
                            <name>Ceph Cluster Space timeleft is less than a day</name>
                            <priority>HIGH</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Space timeleft is less than a hour</name>
                                    <expression>{Template Ceph:ceph.spaceavail.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(3600,,0)}&lt;1h</expression>
                            <name>Ceph Cluster Space timeleft is less than a hour</name>
                            <priority>DISASTER</priority>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(604800,,0)}&lt;1w</expression>
                            <name>Ceph Cluster Space timeleft is less than a week</name>
                            <priority>WARNING</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Space timeleft is less than a hour</name>
                                    <expression>{Template Ceph:ceph.spaceavail.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach FullRatio Mark less than in a day</name>
                                    <expression>{Template Ceph:ceph.fullratio.timeleft(86400,,0)}&lt;1d</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                        <trigger>
                            <expression>{timeleft(2592000,,0)}&lt;30d</expression>
                            <name>Ceph Cluster Space timeleft is less than a week</name>
                            <priority>INFO</priority>
                            <dependencies>
                                <dependency>
                                    <name>Ceph Cluster Space timeleft is less than a hour</name>
                                    <expression>{Template Ceph:ceph.spaceavail.timeleft(3600,,0)}&lt;1h</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Space timeleft is less than a week</name>
                                    <expression>{Template Ceph:ceph.spaceavail.timeleft(604800,,0)}&lt;1w</expression>
                                </dependency>
                                <dependency>
                                    <name>Ceph Cluster Used Space will reach FullRatio Mark less than in a day</name>
                                    <expression>{Template Ceph:ceph.fullratio.timeleft(86400,,0)}&lt;1d</expression>
                                </dependency>
                            </dependencies>
                        </trigger>
                    </triggers>
                </item>
                <item>
                    <name>Ceph Pool Space Total</name>
                    <type>TRAP</type>
                    <key>ceph.spacepooltotal</key>
                    <delay>0</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Space Total</name>
                    <type>TRAP</type>
                    <key>ceph.spacetotal</key>
                    <delay>0</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Space Used</name>
                    <type>CALCULATED</type>
                    <key>ceph.spaceused</key>
                    <delay>5m</delay>
                    <value_type>FLOAT</value_type>
                    <units>Gb</units>
                    <params>last(&quot;ceph.spacetotal&quot;)-last(&quot;ceph.spaceavail&quot;)</params>
                    <applications>
                        <application>
                            <name>Ceph Space</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG stale</name>
                    <type>TRAP</type>
                    <key>ceph.stale</key>
                    <delay>0</delay>
                    <description>The placement group is in an unknown state - the monitors have not received an update for it since the placement group mapping changed.</description>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph PG undersized</name>
                    <type>TRAP</type>
                    <key>ceph.undersized</key>
                    <delay>0</delay>
                    <applications>
                        <application>
                            <name>Ceph pg</name>
                        </application>
                    </applications>
                </item>
                <item>
                    <name>Ceph Write Speed</name>
                    <type>TRAP</type>
                    <key>ceph.wrbps</key>
                    <delay>0</delay>
                    <units>B/s</units>
                    <applications>
                        <application>
                            <name>Ceph Cluster</name>
                        </application>
                    </applications>
                </item>
            </items>
            <discovery_rules>
                <discovery_rule>
                    <name>MON</name>
                    <key>ceph-mon-discovery[{$CLUSTER_NAME},mons,{HOST.HOST}]</key>
                    <item_prototypes>
                        <item_prototype>
                            <name>{#MON}: Status</name>
                            <type>TRAP</type>
                            <key>ceph.monstatus[{#MON}]</key>
                            <delay>0</delay>
                            <applications>
                                <application>
                                    <name>Ceph Cluster</name>
                                </application>
                            </applications>
                            <valuemap>
                                <name>Ceph Daemon Status</name>
                            </valuemap>
                        </item_prototype>
                    </item_prototypes>
                </discovery_rule>
                <discovery_rule>
                    <name>OSD</name>
                    <key>ceph-osd-discovery[{$CLUSTER_NAME},osds,{HOST.HOST}]</key>
                    <item_prototypes>
                        <item_prototype>
                            <name>{#OSD} : Space Available</name>
                            <type>TRAP</type>
                            <key>ceph.osdspaceavail[{#OSD}]</key>
                            <delay>0</delay>
                            <value_type>FLOAT</value_type>
                            <units>Gb</units>
                            <applications>
                                <application>
                                    <name>Ceph Space</name>
                                </application>
                            </applications>
                        </item_prototype>
                        <item_prototype>
                            <name>{#OSD}: Status</name>
                            <type>TRAP</type>
                            <key>ceph.osdstatus[{#OSD}]</key>
                            <delay>0</delay>
                            <applications>
                                <application>
                                    <name>Ceph Cluster</name>
                                </application>
                            </applications>
                            <valuemap>
                                <name>Ceph Daemon Status</name>
                            </valuemap>
                        </item_prototype>
                    </item_prototypes>
                </discovery_rule>
                <discovery_rule>
                    <name>POOL</name>
                    <key>ceph-pools-discovery[{$CLUSTER_NAME},pools,{HOST.HOST}]</key>
                    <item_prototypes>
                        <item_prototype>
                            <name>Space Available : {#POOL}</name>
                            <type>TRAP</type>
                            <key>ceph.poolspaceavail[{#POOL}]</key>
                            <delay>0</delay>
                            <value_type>FLOAT</value_type>
                            <units>Gb</units>
                            <applications>
                                <application>
                                    <name>Ceph Space</name>
                                </application>
                            </applications>
                            <trigger_prototypes>
                                <trigger_prototype>
                                    <expression>{last()}&lt;{$SPACEAVAIL_POOL_HIGH}</expression>
                                    <name>Ceph Cluster: {#POOL} : Space Available is less than {$SPACEAVAIL_POOL_HIGH}</name>
                                    <priority>HIGH</priority>
                                </trigger_prototype>
                            </trigger_prototypes>
                        </item_prototype>
                    </item_prototypes>
                    <graph_prototypes>
                        <graph_prototype>
                            <name>Ceph pool {#POOL} space usage</name>
                            <width>400</width>
                            <height>300</height>
                            <yaxismax>0.0000</yaxismax>
                            <show_work_period>NO</show_work_period>
                            <show_triggers>NO</show_triggers>
                            <type>PIE</type>
                            <show_3d>YES</show_3d>
                            <graph_items>
                                <graph_item>
                                    <color>FF0000</color>
                                    <item>
                                        <host>Template Ceph</host>
                                        <key>ceph.spacepooltotal</key>
                                    </item>
                                </graph_item>
                                <graph_item>
                                    <sortorder>1</sortorder>
                                    <color>00FF00</color>
                                    <item>
                                        <host>Template Ceph</host>
                                        <key>ceph.poolspaceavail[{#POOL}]</key>
                                    </item>
                                </graph_item>
                            </graph_items>
                        </graph_prototype>
                    </graph_prototypes>
                </discovery_rule>
            </discovery_rules>
            <macros>
                <macro>
                    <macro>{$CLUSTER_NAME}</macro>
                    <value>ceph</value>
                </macro>
                <macro>
                    <macro>{$FULLRATIO}</macro>
                    <value>95</value>
                </macro>
                <macro>
                    <macro>{$SPACEAVAIL_AVERAGE}</macro>
                    <value>0.1</value>
                </macro>
                <macro>
                    <macro>{$SPACEAVAIL_HIGH}</macro>
                    <value>0.05</value>
                </macro>
                <macro>
                    <macro>{$SPACEAVAIL_POOL_HIGH}</macro>
                    <value>10</value>
                </macro>
                <macro>
                    <macro>{$SPACEAVAIL_WARNING}</macro>
                    <value>0.2</value>
                </macro>
            </macros>
        </template>
    </templates>
    <triggers>
        <trigger>
            <expression>{Template Ceph:ceph.spaceavail.last()}/{Template Ceph:ceph.spacetotal.last()}&lt;{$SPACEAVAIL_HIGH}</expression>
            <name>Ceph Cluster SpaceAvail is less than 5%</name>
            <priority>HIGH</priority>
        </trigger>
        <trigger>
            <expression>{Template Ceph:ceph.spaceavail.last()}/{Template Ceph:ceph.spacetotal.last()}&lt;{$SPACEAVAIL_AVERAGE}</expression>
            <name>Ceph Cluster SpaceAvail is less than 10%</name>
            <priority>AVERAGE</priority>
        </trigger>
        <trigger>
            <expression>{Template Ceph:ceph.spaceavail.last()}/{Template Ceph:ceph.spacetotal.last()}&lt;{$SPACEAVAIL_WARNING}</expression>
            <name>Ceph Cluster SpaceAvail is less than 15%</name>
            <priority>WARNING</priority>
        </trigger>
    </triggers>
    <graphs>
        <graph>
            <name>Ceph Load</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <graph_items>
                <graph_item>
                    <color>0000C8</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.rdbps</key>
                    </item>
                </graph_item>
                <graph_item>
                    <color>00C800</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.wrbps</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>1</sortorder>
                    <drawtype>FILLED_REGION</drawtype>
                    <color>C80000</color>
                    <yaxisside>RIGHT</yaxisside>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.opsread</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
        <graph>
            <name>Degraded %</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <graph_items>
                <graph_item>
                    <drawtype>GRADIENT_LINE</drawtype>
                    <color>CC0000</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.degraded_percent</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
        <graph>
            <name>Moving PGs</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <graph_items>
                <graph_item>
                    <color>C80000</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.recovering</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>1</sortorder>
                    <color>00C800</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.remapped</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>2</sortorder>
                    <color>0000C8</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.peering</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
        <graph>
            <name>OSDs</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <ymax_type_1>FIXED</ymax_type_1>
            <graph_items>
                <graph_item>
                    <drawtype>GRADIENT_LINE</drawtype>
                    <color>00EE00</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.osd_up</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>1</sortorder>
                    <drawtype>BOLD_LINE</drawtype>
                    <color>CC0000</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.osd_in</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
        <graph>
            <name>PGS</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <graph_items>
                <graph_item>
                    <drawtype>GRADIENT_LINE</drawtype>
                    <color>0000EE</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.active</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>1</sortorder>
                    <drawtype>BOLD_LINE</drawtype>
                    <color>00EE00</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.clean</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
        <graph>
            <name>Problem PGs</name>
            <ymin_type_1>FIXED</ymin_type_1>
            <graph_items>
                <graph_item>
                    <color>00EE00</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.degraded</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>1</sortorder>
                    <color>0000C8</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.incomplete</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>2</sortorder>
                    <color>C800C8</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.inconsistent</key>
                    </item>
                </graph_item>
                <graph_item>
                    <sortorder>3</sortorder>
                    <color>EE0000</color>
                    <item>
                        <host>Template Ceph</host>
                        <key>ceph.down</key>
                    </item>
                </graph_item>
            </graph_items>
        </graph>
    </graphs>
    <value_maps>
        <value_map>
            <name>Ceph Cluster Status</name>
            <mappings>
                <mapping>
                    <value>0</value>
                    <newvalue>HEALTH_ERR</newvalue>
                </mapping>
                <mapping>
                    <value>1</value>
                    <newvalue>HEALTH_OK</newvalue>
                </mapping>
                <mapping>
                    <value>2</value>
                    <newvalue>HEALTH_WARN</newvalue>
                </mapping>
            </mappings>
        </value_map>
        <value_map>
            <name>Ceph Daemon Status</name>
            <mappings>
                <mapping>
                    <value>0</value>
                    <newvalue>Down</newvalue>
                </mapping>
                <mapping>
                    <value>1</value>
                    <newvalue>Up</newvalue>
                </mapping>
                <mapping>
                    <value>2</value>
                    <newvalue>N/A</newvalue>
                </mapping>
            </mappings>
        </value_map>
    </value_maps>
</zabbix_export>
